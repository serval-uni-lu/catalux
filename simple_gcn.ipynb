{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Labeled Transactions-Based Dataset on the Ethereum Network**\n",
    "\n",
    "Link: https://github.com/salam-ammari/Labeled-Transactions-based-Dataset-of-Ethereum-Network/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth = pd.read_csv(\"data/ammari/dataset.csv\")\n",
    "eth.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transaction_graph(df):\n",
    "    \"\"\"\n",
    "    Build a graph from Ethereum transactions\n",
    "    Nodes are addresses, edges are transactions\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    address_info = {}\n",
    "    \n",
    "    # process 'from_address' data\n",
    "    from_addr_df = df[['from_address', 'from_scam', 'from_category']].drop_duplicates()\n",
    "    for _, row in from_addr_df.iterrows():\n",
    "        addr = row['from_address']\n",
    "        if addr not in address_info:\n",
    "            address_info[addr] = {'is_scam': 0, 'category': \"Unknown\"}\n",
    "        \n",
    "        if row['from_scam'] == 1:\n",
    "            address_info[addr]['is_scam'] = 1\n",
    "        \n",
    "        if not pd.isna(row['from_category']):\n",
    "            address_info[addr]['category'] = row['from_category']\n",
    "    \n",
    "    # process 'to_address' data\n",
    "    to_addr_df = df[['to_address', 'to_scam', 'to_category']].drop_duplicates()\n",
    "    for _, row in to_addr_df.iterrows():\n",
    "        addr = row['to_address']\n",
    "        if addr not in address_info:\n",
    "            address_info[addr] = {'is_scam': 0, 'category': \"Unknown\"}\n",
    "        \n",
    "        if row['to_scam'] == 1:\n",
    "            address_info[addr]['is_scam'] = 1\n",
    "        \n",
    "        if not pd.isna(row['to_category']):\n",
    "            address_info[addr]['category'] = row['to_category']\n",
    "    \n",
    "    # add nodes\n",
    "    for addr, info in tqdm(address_info.items(), desc=\"Adding nodes\"):\n",
    "        G.add_node(addr, is_scam=info['is_scam'], category=info['category'])\n",
    "    \n",
    "    # prepare transaction data\n",
    "    edge_data = []\n",
    "    for _, row in tqdm(df.iterrows(), desc=\"Processing transactions\"):\n",
    "        from_addr = row['from_address']\n",
    "        to_addr = row['to_address']\n",
    "        \n",
    "        value = float(row['value']) if not pd.isna(row['value']) else 0.0\n",
    "        gas = float(row['gas']) if not pd.isna(row['gas']) else 0.0\n",
    "        gas_price = float(row['gas_price']) if not pd.isna(row['gas_price']) else 0.0\n",
    "        \n",
    "        timestamp = pd.to_datetime(row['block_timestamp']).timestamp()\n",
    "        \n",
    "        # fraudulent if either sender or receiver is involved in scam\n",
    "        is_fraud = (row['from_scam'] == 1) or (row['to_scam'] == 1)\n",
    "        \n",
    "        edge_data.append((\n",
    "            from_addr,\n",
    "            to_addr,\n",
    "            {\n",
    "                'tx_hash': row['hash'],\n",
    "                'value': value,\n",
    "                'gas': gas,\n",
    "                'gas_price': gas_price,\n",
    "                'timestamp': timestamp,\n",
    "                'is_fraud': int(is_fraud)\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    # add edges\n",
    "    G.add_edges_from(edge_data)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_graph = build_transaction_graph(eth)\n",
    "print(f\"Graph built with {transaction_graph.number_of_nodes()} nodes and {transaction_graph.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_node_features(G):\n",
    "    \"\"\"\n",
    "    Engineer node features for addresses in the transaction graph\n",
    "    \"\"\"\n",
    "    node_features = {} # feature dictionary\n",
    "    \n",
    "    # compute features for each node (address)\n",
    "    for node in G.nodes():\n",
    "        # in and out edges\n",
    "        in_edges = list(G.in_edges(node, data=True))\n",
    "        out_edges = list(G.out_edges(node, data=True))\n",
    "        \n",
    "        # tx count\n",
    "        num_in_tx = len(in_edges)\n",
    "        num_out_tx = len(out_edges)\n",
    "        \n",
    "        # tx value\n",
    "        in_values = [e[2]['value'] for e in in_edges]\n",
    "        out_values = [e[2]['value'] for e in out_edges]\n",
    "        \n",
    "        # aggregate features\n",
    "        total_in_value = sum(in_values) if in_values else 0\n",
    "        total_out_value = sum(out_values) if out_values else 0\n",
    "        avg_in_value = np.mean(in_values) if in_values else 0\n",
    "        avg_out_value = np.mean(out_values) if out_values else 0\n",
    "        max_in_value = max(in_values) if in_values else 0\n",
    "        max_out_value = max(out_values) if out_values else 0\n",
    "        \n",
    "        # time based features\n",
    "        in_timestamps = [e[2]['timestamp'] for e in in_edges]\n",
    "        out_timestamps = [e[2]['timestamp'] for e in out_edges]\n",
    "        \n",
    "        in_intervals = np.diff(sorted(in_timestamps)) if len(in_timestamps) > 1 else [0]\n",
    "        out_intervals = np.diff(sorted(out_timestamps)) if len(out_timestamps) > 1 else [0]\n",
    "        \n",
    "        avg_in_interval = np.mean(in_intervals) if len(in_intervals) > 0 else 0\n",
    "        avg_out_interval = np.mean(out_intervals) if len(out_intervals) > 0 else 0\n",
    "        \n",
    "        # topological features\n",
    "        in_degree = G.in_degree(node)\n",
    "        out_degree = G.out_degree(node)\n",
    "        \n",
    "        # store node features\n",
    "        node_features[node] = {\n",
    "            'num_in_tx': num_in_tx,\n",
    "            'num_out_tx': num_out_tx,\n",
    "            'total_in_value': total_in_value,\n",
    "            'total_out_value': total_out_value,\n",
    "            'avg_in_value': avg_in_value,\n",
    "            'avg_out_value': avg_out_value,\n",
    "            'max_in_value': max_in_value,\n",
    "            'max_out_value': max_out_value,\n",
    "            'avg_in_interval': avg_in_interval,\n",
    "            'avg_out_interval': avg_out_interval,\n",
    "            'in_degree': in_degree,\n",
    "            'out_degree': out_degree,\n",
    "            'net_flow': total_in_value - total_out_value\n",
    "        }\n",
    "    \n",
    "    return node_features\n",
    "\n",
    "def engineer_edge_features(G):\n",
    "    \"\"\"\n",
    "    Engineer edge features for transactions in the graph\n",
    "    \"\"\"\n",
    "\n",
    "    edge_features = {}\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        tx_hash = data['tx_hash']\n",
    "        \n",
    "        # ratio features\n",
    "        sender_total_out = sum([e[2]['value'] for e in G.out_edges(u, data=True)])\n",
    "        receiver_total_in = sum([e[2]['value'] for e in G.in_edges(v, data=True)])\n",
    "        \n",
    "        # pct of sender's outgoing value this transaction represents\n",
    "        pct_of_sender_outgoing = data['value'] / sender_total_out if sender_total_out > 0 else 0\n",
    "        \n",
    "        # pct of receiver's incoming value this transaction represents\n",
    "        pct_of_receiver_incoming = data['value'] / receiver_total_in if receiver_total_in > 0 else 0\n",
    "        \n",
    "        # store edge features\n",
    "        edge_features[(u, v)] = {\n",
    "            'value': data['value'],\n",
    "            'gas': data['gas'],\n",
    "            'gas_price': data['gas_price'],\n",
    "            'timestamp': data['timestamp'],\n",
    "            'pct_of_sender_outgoing': pct_of_sender_outgoing,\n",
    "            'pct_of_receiver_incoming': pct_of_receiver_incoming,\n",
    "            'gas_to_value_ratio': data['gas'] * data['gas_price'] / data['value'] if data['value'] > 0 else 0,\n",
    "            'is_fraud': data['is_fraud']\n",
    "        }\n",
    "    \n",
    "    return edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "node_features = engineer_node_features(transaction_graph)\n",
    "edge_features = engineer_edge_features(transaction_graph)\n",
    "\n",
    "# convert to dataframes\n",
    "node_df = pd.DataFrame.from_dict(node_features, orient='index')\n",
    "edge_df = pd.DataFrame.from_dict(edge_features, orient='index')\n",
    "\n",
    "# scaling\n",
    "node_scaler = StandardScaler()\n",
    "edge_scaler = StandardScaler()\n",
    "\n",
    "node_features_scaled = node_scaler.fit_transform(node_df.fillna(0))\n",
    "edge_features_scaled = edge_scaler.fit_transform(edge_df.drop('is_fraud', axis=1).fillna(0))\n",
    "\n",
    "# add scaled features back to graph\n",
    "for i, node in enumerate(node_df.index):\n",
    "    for j, feature in enumerate(node_df.columns):\n",
    "        transaction_graph.nodes[node][feature] = node_features_scaled[i, j]\n",
    "\n",
    "for i, (u, v) in enumerate(edge_df.index):\n",
    "    for j, feature in enumerate(edge_df.columns.drop('is_fraud')):\n",
    "        transaction_graph.edges[u, v][feature] = edge_features_scaled[i, j]\n",
    "\n",
    "print(\"Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for GNN\n",
    "\n",
    "def prepare_pytorch_geometric_data(G):\n",
    "    \"\"\"\n",
    "    Convert networkx graph to PyTorch Geometric Data object\n",
    "    \"\"\"\n",
    "\n",
    "    # NODES\n",
    "\n",
    "    # node feature matrix\n",
    "    node_list = list(G.nodes())\n",
    "    node_mapping = {node: i for i, node in enumerate(node_list)}\n",
    "    \n",
    "    # first, identify all feature keys across all nodes (excluding labels)\n",
    "    node_feature_keys = set()\n",
    "    for _, attrs in G.nodes(data=True):\n",
    "        node_feature_keys.update([k for k in attrs.keys() if k != 'is_scam' and k != 'category'])\n",
    "    \n",
    "    # sort keys for consistent ordering\n",
    "    node_feature_keys = sorted(list(node_feature_keys))\n",
    "    num_node_features = len(node_feature_keys)\n",
    "    \n",
    "    # initialize feature matrix with the correct dimensions\n",
    "    num_nodes = len(node_list)\n",
    "    x = torch.zeros((num_nodes, num_node_features))\n",
    "    \n",
    "    # node labels (is_scam)\n",
    "    y_nodes = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    \n",
    "    # fill in node features and labels\n",
    "    for i, (node, attrs) in enumerate(G.nodes(data=True)):\n",
    "        for j, key in enumerate(node_feature_keys):\n",
    "            if key in attrs:\n",
    "                x[i, j] = float(attrs[key])  # ensure all values are converted to float\n",
    "        \n",
    "        y_nodes[i] = attrs.get('is_scam', 0)\n",
    "    \n",
    "    # EDGES\n",
    "\n",
    "    # identify all edge feature keys\n",
    "    edge_feature_keys = set()\n",
    "    for _, _, attrs in G.edges(data=True):\n",
    "        edge_feature_keys.update([k for k in attrs.keys() if k != 'is_fraud' and k != 'tx_hash'])\n",
    "    \n",
    "    # sort keys for consistent ordering\n",
    "    edge_feature_keys = sorted(list(edge_feature_keys))\n",
    "    num_edge_features = len(edge_feature_keys)\n",
    "    \n",
    "    # create edge indices and features\n",
    "    edge_list = list(G.edges(data=True))\n",
    "    num_edges = len(edge_list)\n",
    "    \n",
    "    edge_index = torch.zeros((2, num_edges), dtype=torch.long)\n",
    "    edge_attr = torch.zeros((num_edges, num_edge_features))\n",
    "    edge_labels = torch.zeros(num_edges, dtype=torch.long)\n",
    "    \n",
    "    # fill in edge indices, features, and labels\n",
    "    for i, (u, v, attrs) in enumerate(edge_list):\n",
    "        edge_index[0, i] = node_mapping[u]\n",
    "        edge_index[1, i] = node_mapping[v]\n",
    "        \n",
    "        for j, key in enumerate(edge_feature_keys):\n",
    "            if key in attrs:\n",
    "                edge_attr[i, j] = float(attrs[key])  # ensure all values are converted to float\n",
    "        \n",
    "        edge_labels[i] = attrs.get('is_fraud', 0)\n",
    "    \n",
    "    # create PyG\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                y_node=y_nodes, y_edge=edge_labels)\n",
    "    \n",
    "    # store feature names as additional attributes for interpretability\n",
    "    data.node_feature_names = node_feature_keys\n",
    "    data.edge_feature_names = edge_feature_keys\n",
    "    \n",
    "    print(f\"Node features: {num_node_features}, Edge features: {num_edge_features}\")\n",
    "    \n",
    "    return data, node_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_data, node_mapping = prepare_pytorch_geometric_data(transaction_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNFraudDetector(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_channels):\n",
    "        super(GNNFraudDetector, self).__init__()\n",
    "        \n",
    "        # GC layers\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # edge prediction layers\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * hidden_channels + num_edge_features, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(hidden_channels),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(hidden_channels // 2),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_channels // 2, 2)  # binary classification\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # node embedding\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # for each edge, concatenate source node, destination node, and edge features\n",
    "        edge_src, edge_dst = edge_index\n",
    "        edge_features = torch.cat([x[edge_src], x[edge_dst], edge_attr], dim=1)\n",
    "        \n",
    "        # edge classification\n",
    "        edge_pred = self.edge_mlp(edge_features)\n",
    "        \n",
    "        return edge_pred\n",
    "\n",
    "# parameters\n",
    "num_node_features = pyg_data.x.shape[1]\n",
    "num_edge_features = pyg_data.edge_attr.shape[1]\n",
    "hidden_channels = 64\n",
    "\n",
    "# initialize model\n",
    "model = GNNFraudDetector(num_node_features, num_edge_features, hidden_channels)\n",
    "print(f\"Model created with {num_node_features} node features and {num_edge_features} edge features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, epochs=100, lr=0.001):\n",
    "\n",
    "    # split edges into train/val/test\n",
    "    num_edges = data.edge_attr.shape[0]\n",
    "    indices = list(range(num_edges))\n",
    "    \n",
    "    # 70% train, 15% val, 15% test\n",
    "    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # calculate class weights to handle imbalanced data\n",
    "    edge_labels = data.y_edge\n",
    "    num_fraud = (edge_labels == 1).sum().item()\n",
    "    num_non_fraud = (edge_labels == 0).sum().item()\n",
    "    \n",
    "    weight_non_fraud = num_fraud / (num_fraud + num_non_fraud)\n",
    "    weight_fraud = num_non_fraud / (num_fraud + num_non_fraud)\n",
    "    \n",
    "    class_weights = torch.tensor([weight_non_fraud, weight_fraud])\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        edge_pred = model(data.x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        # compute loss on training edges\n",
    "        loss = criterion(edge_pred[train_indices], data.y_edge[train_indices])\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # validation\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(data.x, data.edge_index, data.edge_attr)[val_indices]\n",
    "                val_loss = criterion(val_pred, data.y_edge[val_indices])\n",
    "                \n",
    "                # accuracy\n",
    "                _, predicted = torch.max(val_pred, 1)\n",
    "                val_acc = (predicted == data.y_edge[val_indices]).sum().item() / len(val_indices)\n",
    "            \n",
    "            print(f'Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}')\n",
    "            model.train()\n",
    "    \n",
    "    # test evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(data.x, data.edge_index, data.edge_attr)[test_indices]\n",
    "        test_loss = criterion(test_pred, data.y_edge[test_indices]).item()\n",
    "        \n",
    "        _, predicted = torch.max(test_pred, 1)\n",
    "        test_acc = (predicted == data.y_edge[test_indices]).sum().item() / len(test_indices)\n",
    "        \n",
    "        # calculate precision, recall, F1\n",
    "        true_labels = data.y_edge[test_indices].numpy()\n",
    "        pred_labels = predicted.numpy()\n",
    "        \n",
    "        true_positives = ((pred_labels == 1) & (true_labels == 1)).sum()\n",
    "        false_positives = ((pred_labels == 1) & (true_labels == 0)).sum()\n",
    "        false_negatives = ((pred_labels == 0) & (true_labels == 1)).sum()\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "    \n",
    "    return test_indices, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices, predictions = train_model(model, pyg_data, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fraud_network(G, node_mapping, test_indices, predictions):\n",
    "    \"\"\"\n",
    "    Visualize the transaction graph highlighting detected fraud transactions\n",
    "    \"\"\"\n",
    "    # create subgraph of test transactions\n",
    "    test_edges = []\n",
    "    inv_node_mapping = {v: k for k, v in node_mapping.items()}\n",
    "    \n",
    "    edge_list = list(G.edges())\n",
    "    for i, idx in enumerate(test_indices):\n",
    "        if idx < len(edge_list):\n",
    "            edge = edge_list[idx]\n",
    "            u, v = edge\n",
    "            pred_label = predictions[i].item()\n",
    "            test_edges.append((u, v, {'predicted_fraud': pred_label == 1}))\n",
    "    \n",
    "    test_graph = nx.DiGraph()\n",
    "    test_graph.add_edges_from(test_edges)\n",
    "    \n",
    "    # keep only nodes that are part of test edges\n",
    "    nodes_in_test = set()\n",
    "    for u, v, _ in test_edges:\n",
    "        nodes_in_test.add(u)\n",
    "        nodes_in_test.add(v)\n",
    "    \n",
    "    # color nodes based on scam label\n",
    "    node_colors = []\n",
    "    for node in test_graph.nodes():\n",
    "        if node in G.nodes() and G.nodes[node].get('is_scam', 0) == 1:\n",
    "            node_colors.append('red')\n",
    "        else:\n",
    "            node_colors.append('blue')\n",
    "    \n",
    "    # color edges based on fraud prediction\n",
    "    edge_colors = []\n",
    "    for _, _, attr in test_graph.edges(data=True):\n",
    "        if attr.get('predicted_fraud', False):\n",
    "            edge_colors.append('red')\n",
    "        else:\n",
    "            edge_colors.append('gray')\n",
    "    \n",
    "    # plot the graph\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(test_graph, seed=42)\n",
    "    nx.draw_networkx_nodes(test_graph, pos, node_color=node_colors, node_size=50, alpha=0.8)\n",
    "    nx.draw_networkx_edges(test_graph, pos, edge_color=edge_colors, width=1, alpha=0.6)\n",
    "    \n",
    "    plt.title('Ethereum Transaction Graph with Fraud Predictions')\n",
    "    red_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Scam Address')\n",
    "    blue_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Normal Address')\n",
    "    red_edge = plt.Line2D([0], [0], color='red', lw=2, label='Predicted Fraud Transaction')\n",
    "    gray_edge = plt.Line2D([0], [0], color='gray', lw=2, label='Normal Transaction')\n",
    "    \n",
    "    plt.legend(handles=[red_patch, blue_patch, red_edge, gray_edge], loc='upper right')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fraud_network_visualization.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # analyze top fraud patterns\n",
    "    print(\"\\nAnalyzing fraud patterns...\")\n",
    "    \n",
    "    # get edges predicted as fraud\n",
    "    predicted_fraud_edges = []\n",
    "    for i, idx in enumerate(test_indices):\n",
    "        if predictions[i].item() == 1 and idx < len(edge_list):\n",
    "            edge = edge_list[idx]\n",
    "            predicted_fraud_edges.append(edge)\n",
    "    \n",
    "    # extract features of fraud transactions\n",
    "    if predicted_fraud_edges:\n",
    "        fraud_features = {}\n",
    "        for u, v in predicted_fraud_edges:\n",
    "            edge_data = G.edges[u, v]\n",
    "            for feature, value in edge_data.items():\n",
    "                if feature not in ['tx_hash', 'is_fraud']:\n",
    "                    if feature not in fraud_features:\n",
    "                        fraud_features[feature] = []\n",
    "                    fraud_features[feature].append(value)\n",
    "        \n",
    "        # calculate statistics for fraud transactions\n",
    "        fraud_stats = {}\n",
    "        for feature, values in fraud_features.items():\n",
    "            fraud_stats[feature] = {\n",
    "                'mean': np.mean(values),\n",
    "                'median': np.median(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values)\n",
    "            }\n",
    "        \n",
    "        # print statistics\n",
    "        print(\"\\nFraud Transaction Statistics:\")\n",
    "        for feature, stats in fraud_stats.items():\n",
    "            print(f\"{feature}:\")\n",
    "            for stat_name, stat_value in stats.items():\n",
    "                print(f\"  {stat_name}: {stat_value}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No fraud transactions predicted in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_fraud_network(transaction_graph, node_mapping, test_indices, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and explain feature importance\n",
    "\n",
    "def analyze_feature_importance(model, data, node_mapping):\n",
    "    \"\"\"\n",
    "    Analyze feature importance by measuring gradients\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data.x.requires_grad = True\n",
    "    data.edge_attr.requires_grad = True\n",
    "    \n",
    "    # forward pass\n",
    "    edge_pred = model(data.x, data.edge_index, data.edge_attr)\n",
    "    fraud_prob = edge_pred[:, 1]  # probability of fraud class\n",
    "    \n",
    "    # calculate gradients\n",
    "    fraud_prob.sum().backward()\n",
    "    \n",
    "    # node feature importance\n",
    "    node_feature_importance = data.x.grad.abs().mean(dim=0)\n",
    "    edge_feature_importance = data.edge_attr.grad.abs().mean(dim=0)\n",
    "    \n",
    "    # print feature importance\n",
    "    print(\"\\nNode Feature Importance:\")\n",
    "    node_features = list(node_df.columns)\n",
    "    for i, importance in enumerate(node_feature_importance):\n",
    "        if i < len(node_features):\n",
    "            print(f\"{node_features[i]}: {importance.item():.4f}\")\n",
    "    \n",
    "    print(\"\\nEdge Feature Importance:\")\n",
    "    edge_features = list(edge_df.columns.drop('is_fraud'))\n",
    "    for i, importance in enumerate(edge_feature_importance):\n",
    "        if i < len(edge_features):\n",
    "            print(f\"{edge_features[i]}: {importance.item():.4f}\")\n",
    "    \n",
    "    # save the model\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(model.state_dict(), 'models/eth_fraud_detector_gnn.pt')\n",
    "    print(\"\\nModel saved to 'models/eth_fraud_detector_gnn.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_feature_importance(model, pyg_data, node_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
